{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NumPy Coding Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSR806/iBHubs_AI/blob/main/8.NumPy_Coding_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06f22e03-17d2-45c2-95a4-b617e45a6339"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i9LXloxlIWr"
      },
      "source": [
        "### 1. Macro-averaged F1-score\n",
        "\n",
        "Implement the `macro_averaged_f1_score()` function which computes the Macro-averaged F1 score for a multi-class classification problem.\n",
        "\n",
        "\n",
        "$$\\text{Precision (P) = }\\frac{TP}{TP+FP}$$\n",
        "<br>\n",
        "$$\\text{Recall (R) = }\\frac{TP}{TP+FN}$$\n",
        "<br>\n",
        "$$\\text{F1-score = }\\frac{2PR}{P+R}$$\n",
        "<br>\n",
        "$$\\text{Macro-averaged F1-score = }\\frac{\\text{Sum of F1-scores of all classes }}{\\text{Number of classes}}$$\n",
        "\n",
        "\n",
        "**Arguments**:\n",
        "* **`actual_Y`** :  Actual labels of instances.\n",
        "    * A 1D numpy array of chars\n",
        "    * Array shape: (number of instances, )\n",
        "* **`predicted_Y`**: Predicted labels of instances.\n",
        "    * A 1D numpy array of chars\n",
        "    * Array shape: (number of instances, )\n",
        "    * Assume that `predicted_Y` does not have labels which are not in `actual_Y`\n",
        "\n",
        "\n",
        "**Returns**:\n",
        "Macro-averaged F1-score (A float value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03aeced1-552d-46f5-910b-9f10ff01836f"
      },
      "source": [
        "def macro_averaged_f1_score(actual_Y, predicted_Y):\n",
        "  unique_values = []\n",
        "  for x in actual_Y: \n",
        "    if x not in unique_values:\n",
        "      unique_values.append(x)\n",
        "  for x in predicted_Y: \n",
        "    if x not in unique_values:\n",
        "      unique_values.append(x)\n",
        "  \n",
        "  l = len(unique_values)\n",
        "  confus_mat = np.zeros([l,l])\n",
        "\n",
        "  for i in range(len(actual_Y)):\n",
        "    row = unique_values.index(predicted_Y[i])\n",
        "    col = unique_values.index(actual_Y[i])\n",
        "    confus_mat[row][col] += 1\n",
        "    \n",
        "  p = np.zeros(l,dtype=np.float32)\n",
        "  r = np.zeros(l,dtype=np.float32)\n",
        "  f1 = np.zeros(l,dtype=np.float32)\n",
        "\n",
        "  for i in range(l):\n",
        "    r[i] = confus_mat[i,i]/np.sum(confus_mat[:,i])\n",
        "    p[i] = confus_mat[i,i]/np.sum(confus_mat[i,:])\n",
        "    if r[i]==0 or p[i]==0:\n",
        "      f1[i] = 0\n",
        "    else:\n",
        "      f1[i] = 2*r[i]*p[i]/(r[i] + p[i])\n",
        "\n",
        "  return np.average(f1)\n",
        "  \n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15FKlXP0n8MK",
        "outputId": "24d049e2-9321-419f-f734-aa6f6032e840",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "actual_Y = np.array([\"A\",\"A\",\"B\",\"C\",\"C\"])\n",
        "predicted_Y = np.array([\"A\",\"B\",\"B\",\"C\",\"B\"])\n",
        "f1_score = macro_averaged_f1_score(actual_Y, predicted_Y)\n",
        "print(np.round(f1_score, 3))\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL-PvZB-pKpn"
      },
      "source": [
        "**Expected Output**:\n",
        "```\n",
        "0.611\n",
        "```"
      ]
    }
  ]
}